{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "72b7552c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f06204200f0>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "torch.manual_seed(1357)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5593927",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "15a32f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------- DATA -------------------------\n",
    "\n",
    "with open('./dataset/shakespeare.txt','r',encoding='utf-8') as f:\n",
    "    data = f.read()\n",
    "\n",
    "class CharacterLevelTokenizer:\n",
    "    def __init__(self,data):\n",
    "        self.data = data\n",
    "        self.vocab = sorted(list(set(self.data)))\n",
    "        self.VOCAB_SIZE = len(self.vocab)\n",
    "        \n",
    "        self.i_s = {i:s for i,s in enumerate(self.vocab)}\n",
    "        self.s_i = {s:i for i,s in self.i_s.items()}\n",
    "        \n",
    "    def encode(self,s):\n",
    "        return torch.tensor([self.s_i[c] for c in s],dtype=torch.long)\n",
    "\n",
    "    def decode(self,s):\n",
    "        return ''.join([self.i_s[i.item()] for i in s])\n",
    "\n",
    "tokenizer = CharacterLevelTokenizer(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47696e8a",
   "metadata": {},
   "source": [
    "# Dataset & Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5bcb7da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShakespeareDataset:\n",
    "    def __init__(self,block_size:int, is_test=False) -> None:\n",
    "        self.tokenizer = CharacterLevelTokenizer(data)\n",
    "        self.is_test = is_test\n",
    "        self.full_data = self.tokenizer.encode(self.tokenizer.data)\n",
    "        if self.is_test:\n",
    "            self.data = self.full_data[int(0.9*len(self.full_data)):]\n",
    "        else:\n",
    "            self.data = self.full_data[:int(0.9*len(self.full_data))]\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "\n",
    "    def get_block_size(self) -> int:\n",
    "        return self.block_size\n",
    "\n",
    "    def get_vocab_size(self) -> int:\n",
    "        return self.tokenizer.VOCAB_SIZE\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        item = self.data[idx:idx+self.block_size+1]\n",
    "        x = item[:-1]\n",
    "        y = item[1:]\n",
    "        return x,y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2498fa",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5d8524cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    block_size = 16 # context-length\n",
    "    batch_size = 32 # mini-batch size\n",
    "    vocab_size = tokenizer.VOCAB_SIZE\n",
    "    n_embed = 32\n",
    "    n_heads = 4\n",
    "    head_size = n_embed // n_heads\n",
    "    \n",
    "    n_layers = 5\n",
    "    \n",
    "    eval_iters = 500\n",
    "    lr = 1e-3\n",
    "    \n",
    "    attn_dropout = 0.1\n",
    "    block_dropout = 0.2\n",
    "    \n",
    "    eval_interval = 1000\n",
    "    max_iters = 10_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "24de90da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, Config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.block_size = Config.block_size\n",
    "        self.n_embed = Config.n_embed\n",
    "        self.head_size = Config.head_size\n",
    "        \n",
    "        self.key = nn.Linear(self.n_embed, self.head_size, bias=False)\n",
    "        self.query = nn.Linear(self.n_embed, self.head_size, bias=False)\n",
    "        \n",
    "        self.value = nn.Linear(self.n_embed, self.head_size, bias=False)\n",
    "\n",
    "        self.register_buffer(\n",
    "            'tril',\n",
    "            torch.tril(torch.ones(self.block_size,self.block_size))\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(Config.attn_dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        B,T,C = x.shape\n",
    "\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "\n",
    "        wei = q@k.transpose(-2,-1) * (C ** 0.5)\n",
    "        wei = wei.masked_fill(self.tril[:T,:T]==0,float('-inf'))\n",
    "\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        \n",
    "        wei = self.dropout(wei)\n",
    "        \n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6ffb56",
   "metadata": {},
   "source": [
    "- the projection is added to handle the residual connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "50715eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, Config):\n",
    "        super().__init__()\n",
    "        self.n_heads = Config.n_heads\n",
    "        self.head_size = Config.head_size\n",
    "        \n",
    "        self.heads = nn.ModuleList([AttentionHead(Config) for _ in range(self.n_heads)])\n",
    "        \n",
    "        self.projection = nn.Linear(Config.n_embed, Config.n_embed)\n",
    "        \n",
    "        self.dropout = nn.Dropout(Config.attn_dropout)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = torch.cat([h(x) for h in self.heads],dim=-1)\n",
    "        x = self.projection(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cacc5d0",
   "metadata": {},
   "source": [
    "- the projection is added to handle the residual connections\n",
    "- n_embed is multiplied by 4 as per the paper.\n",
    "\n",
    "dropouts:\n",
    "\n",
    "- in feed forward layer\n",
    "- in multihead attention\n",
    "- in single attention head to dropout the heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c88158ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, Config):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(Config.n_embed,Config.n_embed * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(Config.n_embed * 4, Config.n_embed), # projection\n",
    "            nn.Dropout(Config.block_dropout)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cf31d9",
   "metadata": {},
   "source": [
    "- adding lots of blocks doesn't help since it'll become a large model and the data would trickle down a lot\n",
    "- solution? residual connections!\n",
    "- layernorm: normalize along the rows\n",
    "- instead of normalization after ffwd/multihead_attn, we pre-normalize instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4a15af56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, Config):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(Config)\n",
    "        self.ff = FeedForward(Config)\n",
    "        self.ln1 = nn.LayerNorm(Config.n_embed)\n",
    "        self.ln2 = nn.LayerNorm(Config.n_embed)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.ff(self.ln2(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ff662a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self,Config):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_embed = Config.n_embed\n",
    "        self.block_size = Config.block_size\n",
    "        \n",
    "        self.token_embedding_table = nn.Embedding(Config.vocab_size,self.n_embed)\n",
    "        self.pos_embedding_table = nn.Embedding(self.block_size, self.n_embed)\n",
    "        \n",
    "        self.blocks = nn.Sequential(\n",
    "            *[TransformerBlock(Config)]*Config.n_layers,\n",
    "            nn.LayerNorm(self.n_embed)\n",
    "        )\n",
    "\n",
    "        self.lm_head = nn.Linear(self.n_embed,Config.vocab_size)\n",
    "        \n",
    "    def forward(self,idx,targets=None):\n",
    "        \n",
    "        B,T = idx.shape\n",
    "        \n",
    "        token_embs = self.token_embedding_table(idx)\n",
    "        pos_embs = self.pos_embedding_table(torch.arange(T))\n",
    "        \n",
    "        x = token_embs + pos_embs\n",
    "        x = self.blocks(x)\n",
    "        logits = self.lm_head(x)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B,T,C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits,targets)\n",
    "            \n",
    "        return logits,loss\n",
    "\n",
    "        \n",
    "    def generate(self,idx,total):\n",
    "        for _ in range(total):\n",
    "            idx_cond = idx[:, -self.block_size:]\n",
    "            logits, loss = self(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "7ee7f5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ShakespeareDataset(Config.block_size)\n",
    "val_ds = ShakespeareDataset(Config.block_size,is_test=True)\n",
    "\n",
    "train_dl = torch.utils.data.DataLoader(train_ds,shuffle=False,batch_size=Config.batch_size)\n",
    "val_dl = torch.utils.data.DataLoader(val_ds,shuffle=False,batch_size=Config.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "089255e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = LanguageModel(Config)\n",
    "optim = torch.optim.AdamW(lm.parameters(),lr=Config.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "40d01131",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    lm.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(Config.eval_iters)\n",
    "        if split=='train':\n",
    "            it = iter(train_dl)\n",
    "        else:\n",
    "            it = iter(val_dl)\n",
    "        for k in range(Config.eval_iters):\n",
    "            X, Y = next(it)\n",
    "            logits, loss = lm(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    lm.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "af2367f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.2709, val loss 4.2514\n",
      "step 1000: train loss 2.4884, val loss 2.6560\n",
      "step 2000: train loss 2.3917, val loss 2.6376\n",
      "step 3000: train loss 2.2830, val loss 2.5369\n",
      "step 4000: train loss 2.2121, val loss 2.5069\n",
      "step 5000: train loss 2.2196, val loss 2.3605\n",
      "step 6000: train loss 2.2704, val loss 2.3862\n",
      "step 7000: train loss 2.2593, val loss 2.3365\n",
      "step 8000: train loss 2.2940, val loss 2.3538\n",
      "step 9000: train loss 2.3210, val loss 2.3379\n"
     ]
    }
   ],
   "source": [
    "train_iter = iter(train_dl)\n",
    "\n",
    "for step in range(10_000):\n",
    "    inputs,targets = next(train_iter)\n",
    "    logits,loss=lm(inputs,targets)\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    if step % Config.eval_interval == 0 or step == Config.max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {step}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "9d5b8e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated (500 tokens) >>>\n",
      " \n",
      "KING:\n",
      "If fly cimeen leth yould mANT:\n",
      "Saybenter ands And\n",
      "The din hich teich hern and fmaigh in tailef,\n",
      "And ine epeeplectiet the cuure beutintish.\n",
      "\n",
      "ORDY:\n",
      "And roont ans ans of all StireZstids to to the whe tie youen is to wou tho thoulcel Ritent mred\n",
      "HaiNG RITHAHAEENTHANY:\n",
      "My achaS:\n",
      "Wes ysis\n",
      "O doreus ingre in\n",
      "Aydircours dower's to hee lood'd as daidf Endind a wing my\n",
      "I itis mortwacht abruevanrings ulist wo sande,\n",
      "Claiine par\n",
      "Or trin.\n",
      "\n",
      "Swo the sralls witt thy am ENTHARD Ritind:\n",
      "In dakel,\n",
      "Hightlust, \n"
     ]
    }
   ],
   "source": [
    "generated = lm.generate(\n",
    "    torch.zeros((1,1),dtype=torch.long), # initial context 0\n",
    "    total=500\n",
    ")\n",
    "generated = tokenizer.decode(generated[0])\n",
    "print('generated (500 tokens) >>>\\n',generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b2527d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
